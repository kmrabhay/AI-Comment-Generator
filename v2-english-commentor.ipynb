{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an example on how to fine tune mT5 model with Higgingface Transformers to solve multilingual task in 101 lanaguges. This notebook especially takes the problem of question generation in hindi lanagues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:16.428911Z",
     "iopub.status.busy": "2022-11-24T10:02:16.428501Z",
     "iopub.status.idle": "2022-11-24T10:02:19.127841Z",
     "shell.execute_reply": "2022-11-24T10:02:19.126772Z",
     "shell.execute_reply.started": "2022-11-24T10:02:16.428877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.empty_cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.135002Z",
     "iopub.status.busy": "2022-11-24T10:02:19.132712Z",
     "iopub.status.idle": "2022-11-24T10:02:19.141174Z",
     "shell.execute_reply": "2022-11-24T10:02:19.140200Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.134918Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.148308Z",
     "iopub.status.busy": "2022-11-24T10:02:19.145672Z",
     "iopub.status.idle": "2022-11-24T10:02:19.175070Z",
     "shell.execute_reply": "2022-11-24T10:02:19.174168Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.146374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only-english-mt5-dataset/valid.csv\n",
      "only-english-mt5-dataset/train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('only-english-mt5-dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.184115Z",
     "iopub.status.busy": "2022-11-24T10:02:19.183469Z",
     "iopub.status.idle": "2022-11-24T10:02:19.192181Z",
     "shell.execute_reply": "2022-11-24T10:02:19.189751Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.184076Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"expand_frame_repr\", False) # print cols side by side as it's supposed to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T11:24:20.806693Z",
     "iopub.status.busy": "2022-11-24T11:24:20.806313Z",
     "iopub.status.idle": "2022-11-24T11:24:20.841532Z",
     "shell.execute_reply": "2022-11-24T11:24:20.840832Z",
     "shell.execute_reply.started": "2022-11-24T11:24:20.806660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68372, 4)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"only-english-mt5-dataset/train.csv\")\n",
    "# train_path = \"../input/only-english-mt5-dataset/train.csv\"\n",
    "# val_path = \"../input/only-english-mt5-dataset/valid.csv\"\n",
    "\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.210735Z",
     "iopub.status.busy": "2022-11-24T10:02:19.210358Z",
     "iopub.status.idle": "2022-11-24T10:02:19.218788Z",
     "shell.execute_reply": "2022-11-24T10:02:19.217860Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.210698Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.224366Z",
     "iopub.status.busy": "2022-11-24T10:02:19.222532Z",
     "iopub.status.idle": "2022-11-24T10:02:19.230428Z",
     "shell.execute_reply": "2022-11-24T10:02:19.229290Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.224326Z"
    }
   },
   "outputs": [],
   "source": [
    "# train[\"comment\"].head(10).apply(isEnglish)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.234039Z",
     "iopub.status.busy": "2022-11-24T10:02:19.232180Z",
     "iopub.status.idle": "2022-11-24T10:02:19.242183Z",
     "shell.execute_reply": "2022-11-24T10:02:19.241180Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.234001Z"
    }
   },
   "outputs": [],
   "source": [
    "# train['comment'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.244178Z",
     "iopub.status.busy": "2022-11-24T10:02:19.243520Z",
     "iopub.status.idle": "2022-11-24T10:02:19.252638Z",
     "shell.execute_reply": "2022-11-24T10:02:19.251873Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.244139Z"
    }
   },
   "outputs": [],
   "source": [
    "# train = processEnglish(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.254967Z",
     "iopub.status.busy": "2022-11-24T10:02:19.254384Z",
     "iopub.status.idle": "2022-11-24T10:02:19.265021Z",
     "shell.execute_reply": "2022-11-24T10:02:19.264092Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.254930Z"
    }
   },
   "outputs": [],
   "source": [
    "# train = train[train.nonEng<0.35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.266493Z",
     "iopub.status.busy": "2022-11-24T10:02:19.266034Z",
     "iopub.status.idle": "2022-11-24T10:02:19.276027Z",
     "shell.execute_reply": "2022-11-24T10:02:19.275034Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.266459Z"
    }
   },
   "outputs": [],
   "source": [
    "# train[train.nonEng<0.35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.277517Z",
     "iopub.status.busy": "2022-11-24T10:02:19.277182Z",
     "iopub.status.idle": "2022-11-24T10:02:19.288963Z",
     "shell.execute_reply": "2022-11-24T10:02:19.287944Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.277482Z"
    }
   },
   "outputs": [],
   "source": [
    "# del train['nonEng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:19.290470Z",
     "iopub.status.busy": "2022-11-24T10:02:19.290108Z",
     "iopub.status.idle": "2022-11-24T10:02:28.317416Z",
     "shell.execute_reply": "2022-11-24T10:02:28.316610Z",
     "shell.execute_reply.started": "2022-11-24T10:02:19.290436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pytorch_lightning==1.0.6 in ./t5_venv/lib/python3.8/site-packages (1.0.6)\n",
      "Requirement already satisfied: PyYAML>=5.1 in ./t5_venv/lib/python3.8/site-packages (from pytorch_lightning==1.0.6) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in ./t5_venv/lib/python3.8/site-packages (from pytorch_lightning==1.0.6) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in ./t5_venv/lib/python3.8/site-packages (from pytorch_lightning==1.0.6) (2022.11.0)\n",
      "Requirement already satisfied: torch>=1.3 in ./t5_venv/lib/python3.8/site-packages (from pytorch_lightning==1.0.6) (1.13.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in ./t5_venv/lib/python3.8/site-packages (from pytorch_lightning==1.0.6) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.16.4 in ./t5_venv/lib/python3.8/site-packages (from pytorch_lightning==1.0.6) (1.23.5)\n",
      "Requirement already satisfied: future>=0.17.1 in ./t5_venv/lib/python3.8/site-packages (from pytorch_lightning==1.0.6) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in ./t5_venv/lib/python3.8/site-packages (from torch>=1.3->pytorch_lightning==1.0.6) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./t5_venv/lib/python3.8/site-packages (from torch>=1.3->pytorch_lightning==1.0.6) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./t5_venv/lib/python3.8/site-packages (from torch>=1.3->pytorch_lightning==1.0.6) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./t5_venv/lib/python3.8/site-packages (from torch>=1.3->pytorch_lightning==1.0.6) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./t5_venv/lib/python3.8/site-packages (from torch>=1.3->pytorch_lightning==1.0.6) (11.10.3.66)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (2.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (44.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (1.50.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (1.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (0.38.4)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (3.20.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./t5_venv/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning==1.0.6) (0.4.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./t5_venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./t5_venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./t5_venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in ./t5_venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./t5_venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./t5_venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./t5_venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./t5_venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (3.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in ./t5_venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./t5_venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (2.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./t5_venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./t5_venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in ./t5_venv/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (3.10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./t5_venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.0.6) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "!pip install pytorch_lightning==1.0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:28.320946Z",
     "iopub.status.busy": "2022-11-24T10:02:28.320665Z",
     "iopub.status.idle": "2022-11-24T10:02:33.714110Z",
     "shell.execute_reply": "2022-11-24T10:02:33.712979Z",
     "shell.execute_reply.started": "2022-11-24T10:02:28.320917Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentencepiece in ./t5_venv/lib/python3.8/site-packages (0.1.97)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:02:33.720900Z",
     "iopub.status.busy": "2022-11-24T10:02:33.718853Z",
     "iopub.status.idle": "2022-11-24T10:03:16.928738Z",
     "shell.execute_reply": "2022-11-24T10:03:16.927778Z",
     "shell.execute_reply.started": "2022-11-24T10:02:33.720855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already up-to-date: transformers in ./t5_venv/lib/python3.8/site-packages (4.24.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in ./t5_venv/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: requests in ./t5_venv/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in ./t5_venv/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.10.0 in ./t5_venv/lib/python3.8/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in ./t5_venv/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in ./t5_venv/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./t5_venv/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: filelock in ./t5_venv/lib/python3.8/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in ./t5_venv/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in ./t5_venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in ./t5_venv/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in ./t5_venv/lib/python3.8/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<3,>=2 in ./t5_venv/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in ./t5_venv/lib/python3.8/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in ./t5_venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:16.932359Z",
     "iopub.status.busy": "2022-11-24T10:03:16.932070Z",
     "iopub.status.idle": "2022-11-24T10:03:24.503518Z",
     "shell.execute_reply": "2022-11-24T10:03:24.502655Z",
     "shell.execute_reply.started": "2022-11-24T10:03:16.932328Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:24.505214Z",
     "iopub.status.busy": "2022-11-24T10:03:24.504894Z",
     "iopub.status.idle": "2022-11-24T10:03:24.618890Z",
     "shell.execute_reply": "2022-11-24T10:03:24.618026Z",
     "shell.execute_reply.started": "2022-11-24T10:03:24.505180Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:24.622365Z",
     "iopub.status.busy": "2022-11-24T10:03:24.622100Z",
     "iopub.status.idle": "2022-11-24T10:03:24.626550Z",
     "shell.execute_reply": "2022-11-24T10:03:24.625570Z",
     "shell.execute_reply.started": "2022-11-24T10:03:24.622338Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be pytorch-lightning library for training. Most of the below code is adapted from here https://github.com/huggingface/transformers/blob/master/examples/lightning_base.py\n",
    "\n",
    "The trainer is generic and can be used for any text-2-text task. You'll just need to change the dataset. Rest of the code will stay unchanged for all the tasks.\n",
    "\n",
    "This is the most intresting and powrfull thing about the text-2-text format. You can fine-tune the model on variety of NLP tasks by just formulating the problem in text-2-text setting. No need to change hyperparameters, learning rate, optimizer or loss function. Just plug in your dataset and you are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:24.628590Z",
     "iopub.status.busy": "2022-11-24T10:03:24.628016Z",
     "iopub.status.idle": "2022-11-24T10:03:24.655918Z",
     "shell.execute_reply": "2022-11-24T10:03:24.655168Z",
     "shell.execute_reply.started": "2022-11-24T10:03:24.628537Z"
    }
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "#         self.hparams = hparams\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "\n",
    "    def is_logger(self):\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        labels = batch[\"target_ids\"]\n",
    "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "#     def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "#         if self.trainer.use_tpu:\n",
    "#             xm.optimizer_step(optimizer)\n",
    "#         else:\n",
    "#             optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         self.lr_scheduler.step()\n",
    "\n",
    "    def optimizer_step(self,\n",
    "                     epoch=None,\n",
    "                     batch_idx=None,\n",
    "                     optimizer=None,\n",
    "                     optimizer_idx=None,\n",
    "                     optimizer_closure=None,\n",
    "                     on_tpu=None,\n",
    "                     using_native_amp=None,\n",
    "                     using_lbfgs=None):\n",
    "        optimizer.step() # remove 'closure=optimizer_closure' here\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
    "                                num_workers=4)\n",
    "        t_total = (\n",
    "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "                // self.hparams.gradient_accumulation_steps\n",
    "                * float(self.hparams.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"valid\", args=self.hparams)\n",
    "        print(\"val data set: \", len(val_dataset))\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:24.657684Z",
     "iopub.status.busy": "2022-11-24T10:03:24.657289Z",
     "iopub.status.idle": "2022-11-24T10:03:24.670993Z",
     "shell.execute_reply": "2022-11-24T10:03:24.670126Z",
     "shell.execute_reply.started": "2022-11-24T10:03:24.657640Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "        def on_validation_end(self, trainer, pl_module):\n",
    "            logger.info(\"***** Validation results *****\")\n",
    "            if pl_module.is_logger():\n",
    "                  metrics = trainer.callback_metrics\n",
    "                  # Log results\n",
    "                  for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                      logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "        def on_test_end(self, trainer, pl_module):\n",
    "            logger.info(\"***** Test results *****\")\n",
    "\n",
    "            if pl_module.is_logger():\n",
    "                metrics = trainer.callback_metrics\n",
    "\n",
    "                  # Log and save results to file\n",
    "                output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "                with open(output_test_results_file, \"w\") as writer:\n",
    "                    for key in sorted(metrics):\n",
    "                          if key not in [\"log\", \"progress_bar\"]:\n",
    "                            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the hyperparameters and other arguments. You can overide this dict for specific task as needed. While in most of cases you'll only need to change the data_dirand output_dir.\n",
    "\n",
    "Here the batch size is 8 and gradient_accumulation_steps are 8 so the effective batch size is 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:24.672681Z",
     "iopub.status.busy": "2022-11-24T10:03:24.672270Z",
     "iopub.status.idle": "2022-11-24T10:03:24.683132Z",
     "shell.execute_reply": "2022-11-24T10:03:24.682320Z",
     "shell.execute_reply.started": "2022-11-24T10:03:24.672632Z"
    }
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='google/mt5-base',\n",
    "    tokenizer_name_or_path='google/mt5-base',\n",
    "    max_seq_length=512,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=16,\n",
    "    eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:24.685097Z",
     "iopub.status.busy": "2022-11-24T10:03:24.684745Z",
     "iopub.status.idle": "2022-11-24T10:03:25.776805Z",
     "shell.execute_reply": "2022-11-24T10:03:25.775950Z",
     "shell.execute_reply.started": "2022-11-24T10:03:24.685063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ls\t\t\t  t5_venv    v2-english-commentor.ipynb  valid.csv\n",
      "only-english-mt5-dataset  train.csv  v2-english-commentor.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:03:25.780065Z",
     "iopub.status.busy": "2022-11-24T10:03:25.779781Z",
     "iopub.status.idle": "2022-11-24T10:03:26.722640Z",
     "shell.execute_reply": "2022-11-24T10:03:26.721724Z",
     "shell.execute_reply.started": "2022-11-24T10:03:25.780034Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:25:26.602104Z",
     "iopub.status.busy": "2022-11-24T10:25:26.601728Z",
     "iopub.status.idle": "2022-11-24T10:25:26.616382Z",
     "shell.execute_reply": "2022-11-24T10:25:26.615286Z",
     "shell.execute_reply.started": "2022-11-24T10:25:26.602073Z"
    }
   },
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, type_path, max_len=30):\n",
    "        self.path = os.path.join(data_dir, type_path + '.csv')\n",
    "\n",
    "        self.title = 'title'\n",
    "        self.comment = 'comment'\n",
    "        self.category = 'category'\n",
    "        print(\"----1\")\n",
    "        self.data = pd.read_csv(self.path)\n",
    "        print(\"---2--\", len(self.data))\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            input_text_1,input_text_2,output_text= self.data.loc[idx, self.title], self.data.loc[idx, self.category],self.data.loc[idx, self.comment]\n",
    "   \n",
    "            input_ = \"Title: %s Category: %s\" % (input_text_1,input_text_2)\n",
    "            target = \"%s \" %(output_text)\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=45, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=45, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:23:42.095955Z",
     "iopub.status.busy": "2022-11-24T10:23:42.095622Z",
     "iopub.status.idle": "2022-11-24T10:23:43.406535Z",
     "shell.execute_reply": "2022-11-24T10:23:43.405670Z",
     "shell.execute_reply.started": "2022-11-24T10:23:42.095925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhay/T5/t5_venv/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:25:30.100086Z",
     "iopub.status.busy": "2022-11-24T10:25:30.099753Z",
     "iopub.status.idle": "2022-11-24T10:26:00.335025Z",
     "shell.execute_reply": "2022-11-24T10:26:00.333385Z",
     "shell.execute_reply.started": "2022-11-24T10:25:30.100054Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----1\n",
      "---2-- 68372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhay/T5/t5_venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val dataset:  68372\n"
     ]
    }
   ],
   "source": [
    "dataset = CommentDataset(tokenizer, 'only-english-mt5-dataset', 'train', 30)\n",
    "print(\"Val dataset: \",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T11:06:54.162476Z",
     "iopub.status.busy": "2022-11-24T11:06:54.162086Z",
     "iopub.status.idle": "2022-11-24T11:06:54.170807Z",
     "shell.execute_reply": "2022-11-24T11:06:54.169895Z",
     "shell.execute_reply.started": "2022-11-24T11:06:54.162440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 30 min Full Body Fat Burn HIIT (NO JUMPING) - Ab, Core, Arm, Back, Leg, Thigh & Cardio ~ Emi Category: Fitness</s><pad><pad><pad><pad><pad><pad><pad>\n",
      "how many calories burn for thse type of workout </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "data = dataset[20]\n",
    "print(tokenizer.decode(data['source_ids']))\n",
    "print(tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.075795Z",
     "iopub.status.idle": "2022-11-24T10:03:31.076195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.077524Z",
     "iopub.status.idle": "2022-11-24T10:03:31.078277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.6\n"
     ]
    }
   ],
   "source": [
    "print(pl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:26:31.412667Z",
     "iopub.status.busy": "2022-11-24T10:26:31.412237Z",
     "iopub.status.idle": "2022-11-24T10:26:31.421364Z",
     "shell.execute_reply": "2022-11-24T10:26:31.420325Z",
     "shell.execute_reply.started": "2022-11-24T10:26:31.412628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': 'only-english-mt5-dataset', 'output_dir': 'working/result', 'model_name_or_path': 'google/mt5-base', 'tokenizer_name_or_path': 'google/mt5-base', 'max_seq_length': 30, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 16, 'eval_batch_size': 16, 'num_train_epochs': 10, 'gradient_accumulation_steps': 16, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "args_dict.update({'data_dir': 'only-english-mt5-dataset', 'output_dir': 'working/result', 'num_train_epochs':10,'max_seq_length':30})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:26:33.580941Z",
     "iopub.status.busy": "2022-11-24T10:26:33.580557Z",
     "iopub.status.idle": "2022-11-24T10:26:33.622829Z",
     "shell.execute_reply": "2022-11-24T10:26:33.621684Z",
     "shell.execute_reply.started": "2022-11-24T10:26:33.580910Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    period =1,filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=2,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:26:35.498262Z",
     "iopub.status.busy": "2022-11-24T10:26:35.497928Z",
     "iopub.status.idle": "2022-11-24T10:26:35.503126Z",
     "shell.execute_reply": "2022-11-24T10:26:35.502230Z",
     "shell.execute_reply.started": "2022-11-24T10:26:35.498232Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "    return CommentDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:26:37.473613Z",
     "iopub.status.busy": "2022-11-24T10:26:37.473257Z",
     "iopub.status.idle": "2022-11-24T10:28:19.639646Z",
     "shell.execute_reply": "2022-11-24T10:28:19.638705Z",
     "shell.execute_reply.started": "2022-11-24T10:26:37.473557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize model Namespace(adam_epsilon=1e-08, data_dir='only-english-mt5-dataset', early_stop_callback=False, eval_batch_size=16, fp_16=False, gradient_accumulation_steps=16, learning_rate=0.0003, max_grad_norm=1.0, max_seq_length=30, model_name_or_path='google/mt5-base', n_gpu=1, num_train_epochs=10, opt_level='O1', output_dir='working/result', seed=42, tokenizer_name_or_path='google/mt5-base', train_batch_size=16, warmup_steps=0, weight_decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhay/T5/t5_venv/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    }
   ],
   "source": [
    "print (\"Initialize model\", args)\n",
    "model = T5FineTuner(args)\n",
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:29:51.257705Z",
     "iopub.status.busy": "2022-11-24T10:29:51.257230Z",
     "iopub.status.idle": "2022-11-24T10:29:51.277045Z",
     "shell.execute_reply": "2022-11-24T10:29:51.276114Z",
     "shell.execute_reply.started": "2022-11-24T10:29:51.257658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "torch.cuda.device_count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:29:53.582191Z",
     "iopub.status.busy": "2022-11-24T10:29:53.581841Z",
     "iopub.status.idle": "2022-11-24T10:29:53.606428Z",
     "shell.execute_reply": "2022-11-24T10:29:53.605470Z",
     "shell.execute_reply.started": "2022-11-24T10:29:53.582159Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:29:58.332250Z",
     "iopub.status.busy": "2022-11-24T10:29:58.331893Z",
     "iopub.status.idle": "2022-11-24T10:29:58.337019Z",
     "shell.execute_reply": "2022-11-24T10:29:58.335995Z",
     "shell.execute_reply.started": "2022-11-24T10:29:58.332216Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:29:59.948145Z",
     "iopub.status.busy": "2022-11-24T10:29:59.946402Z",
     "iopub.status.idle": "2022-11-24T10:30:12.109801Z",
     "shell.execute_reply": "2022-11-24T10:30:12.107705Z",
     "shell.execute_reply.started": "2022-11-24T10:29:59.948092Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install GPUtil\n",
    "\n",
    "# import torch\n",
    "# from GPUtil import showUtilization as gpu_usage\n",
    "# from numba import cuda\n",
    "\n",
    "# def free_gpu_cache():\n",
    "#     print(\"Initial GPU Usage\")\n",
    "#     gpu_usage()                             \n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     cuda.select_device(0)\n",
    "#     cuda.close()\n",
    "#     cuda.select_device(0)\n",
    "\n",
    "#     print(\"GPU Usage after emptying the cache\")\n",
    "#     gpu_usage()\n",
    "\n",
    "# free_gpu_cache()                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T10:30:19.804285Z",
     "iopub.status.busy": "2022-11-24T10:30:19.803910Z",
     "iopub.status.idle": "2022-11-24T10:31:13.056871Z",
     "shell.execute_reply": "2022-11-24T10:31:13.053710Z",
     "shell.execute_reply.started": "2022-11-24T10:30:19.804249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training model\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "Trainer(distributed_backend=gpu is not a supported backend",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [105], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Training model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/T5/t5_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:427\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPL_TESTING_MODE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# SET UP TRAINING\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_accelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_backend\u001b[38;5;241m.\u001b[39msetup(model)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# INSPECT THESE FOR MAIN LOOPS\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# assign training and eval functions... inspect these to see the train and eval loops :)\u001b[39;00m\n",
      "File \u001b[0;32m~/T5/t5_venv/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator_connector.py:288\u001b[0m, in \u001b[0;36mAcceleratorConnector.select_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m     accelerator_backend \u001b[38;5;241m=\u001b[39m accelerators\u001b[38;5;241m.\u001b[39mCPUAccelerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, cluster_env)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrainer(distributed_backend=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mdistributed_backend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a supported backend\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accelerator_backend\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: Trainer(distributed_backend=gpu is not a supported backend"
     ]
    }
   ],
   "source": [
    "print (\" Training model\")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "print (\"training finished\")\n",
    "\n",
    "print (\"Saving model\")\n",
    "\n",
    "model.model.save_pretrained(\"/kaggle/working/result\")\n",
    "\n",
    "print (\"Saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.097508Z",
     "iopub.status.idle": "2022-11-24T10:03:31.098298Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.099456Z",
     "iopub.status.idle": "2022-11-24T10:03:31.100216Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.103343Z",
     "iopub.status.idle": "2022-11-24T10:03:31.104146Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.105352Z",
     "iopub.status.idle": "2022-11-24T10:03:31.106155Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.107378Z",
     "iopub.status.idle": "2022-11-24T10:03:31.108228Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.109466Z",
     "iopub.status.idle": "2022-11-24T10:03:31.110334Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.111630Z",
     "iopub.status.idle": "2022-11-24T10:03:31.112437Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.113756Z",
     "iopub.status.idle": "2022-11-24T10:03:31.114625Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.116077Z",
     "iopub.status.idle": "2022-11-24T10:03:31.117029Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-24T10:03:31.118321Z",
     "iopub.status.idle": "2022-11-24T10:03:31.119174Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T11:07:03.781205Z",
     "iopub.status.busy": "2022-11-24T11:07:03.780875Z",
     "iopub.status.idle": "2022-11-24T11:07:06.253538Z",
     "shell.execute_reply": "2022-11-24T11:07:06.252527Z",
     "shell.execute_reply.started": "2022-11-24T11:07:03.781174Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
